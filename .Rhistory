return(DF)
}
#
# perform all data parsing in one function
#
parseTripData <- function(DF) {
DF <- DF[colsToKeep]
DF <- renameCols(DF)
DF <- filterTrips(DF)
DF <- encodeCompanies(DF)
DF <- formatDate(DF)
DF <- fixGranularity(DF)
return(DF)
}
######################################################################
#                       "MAIN" METHOD                                #
######################################################################
getData <- function() {
# assign file names for 165 files
filenames <- list()
for (i in 1:165) {
filenames[i] <- paste("./datasets/Taxi_Trips_-_2019_", i, ".tsv", sep='')
}
# get first dataframe with header
DF <- read.table(filenames[[1]], sep = ",", header = TRUE)
DF <- parseTripData(DF)
# append dataframes without header to original dataframe
for (i in 2:165) {
# read next csv
temp <- read.table(file = paste0(filenames[[i]]), sep = ",", header = FALSE)
temp <- parseTripData(temp)
print(paste("parsing file: ", i))
# append to main dataframe
DF <- rbind(DF, temp)
}
return(DF)
}
DF <- getData()
gc()
#
# SET UP DATA FILE AND EXCLUDE UNWANTED DATA
#
library(dplyr)
library(hash)
library(lubridate)
library(stringr)
source("./companies.r", local = FALSE) # for hashing company names into abbreviations
colsToKeep <- c(3, 5, 6, 9, 10, 17)
easyColNames <- c("start", "duration", "miles", "pickupArea", "dropOffArea", "Company")
######################################################################
#                       HELPER METHODS                               #
######################################################################
#
# remove all trips that do not fall under constraints
#
filterTrips <- function(DF) {
# remove less than 0.5 miles & more than 100 miles
DF <- subset(DF, miles >= .5 & miles <= 100.00)
# remove less than 60 seconds & greater than 5 hours
DF <- subset(DF, duration >= 60 & duration <= 60 * 60 * 5)  # 1-4 312969 rows
# all trips that either start or end outside of a Chicago community area
DF <- subset(DF, !is.na(pickupArea) & !is.na(dropOffArea))  # 1-4 28676 rows
}
#
# give easy names to columns
#
renameCols <- function(DF) {
names(DF) <- easyColNames
return(DF)
}
#
# format to nice date format
#
formatDate <- function(DF) {
DF$start <- strptime(DF$start, format='%m/%d/%Y %I:%M:%S %p')
return(DF)
}
#
# fix trips granularity - 12:15, 12:30, 12:45 become 12:00
#
fixGranularity <- function (DF) {
DF$start <- as.POSIXct(paste0(format(DF$start,"%Y-%m-%d %H:"), "00:00 CDT"))
return(DF)
}
#
# encode companies - each company gets a code
#
encodeCompanies <- function(DF) {
DF$Company <- apply(FUN = function(row) compHash[[row["Company"]]], X = DF, MARGIN = 1)
return(DF)
}
#
# perform all data parsing in one function
#
parseTripData <- function(DF) {
DF <- DF[colsToKeep]
DF <- renameCols(DF)
DF <- filterTrips(DF)
DF <- encodeCompanies(DF)
DF <- formatDate(DF)
DF <- fixGranularity(DF)
return(DF)
}
######################################################################
#                       "MAIN" METHOD                                #
######################################################################
getData <- function() {
# assign file names for 165 files
filenames <- list()
for (i in 1:165) {
filenames[i] <- paste("./datasets/Taxi_Trips_-_2019_", i, ".tsv", sep='')
}
# get first dataframe with header
DF <- read.table(filenames[[1]], sep = ",", header = TRUE)
DF <- parseTripData(DF)
# append dataframes without header to original dataframe
for (i in 103:105) {
# read next csv
temp <- read.table(file = paste0(filenames[[i]]), sep = ",", header = FALSE)
temp <- parseTripData(temp)
print(paste("parsing file: ", i))
# append to main dataframe
DF <- rbind(DF, temp)
}
return(DF)
}
DF <- getData()
shiny::runApp()
runApp()
runApp()
getBasicBarPlot(parseByHour(DF))
parseByHour(DF)
#
# SET UP DATA FILE AND EXCLUDE UNWANTED DATA
#
library(dplyr)
library(hash)
library(lubridate)
library(stringr)
source("./companies.r", local = FALSE) # for hashing company names into abbreviations
colsToKeep <- c(3, 5, 6, 9, 10, 17)
easyColNames <- c("start", "duration", "miles", "pickupArea", "dropOffArea", "Company")
######################################################################
#                       HELPER METHODS                               #
######################################################################
#
# remove all trips that do not fall under constraints
#
filterTrips <- function(DF) {
# remove less than 0.5 miles & more than 100 miles
DF <- subset(DF, miles >= .5 & miles <= 100.00)
# remove less than 60 seconds & greater than 5 hours
DF <- subset(DF, duration >= 60 & duration <= 60 * 60 * 5)  # 1-4 312969 rows
# all trips that either start or end outside of a Chicago community area
DF <- subset(DF, !is.na(pickupArea) & !is.na(dropOffArea))  # 1-4 28676 rows
}
#
# give easy names to columns
#
renameCols <- function(DF) {
names(DF) <- easyColNames
return(DF)
}
#
# format to nice date format
#
formatDate <- function(DF) {
DF$start <- strptime(DF$start, format='%m/%d/%Y %I:%M:%S %p')
return(DF)
}
#
# fix trips granularity - 12:15, 12:30, 12:45 become 12:00
#
fixGranularity <- function (DF) {
DF$start <- as.POSIXct(paste0(format(DF$start,"%Y-%m-%d %H:"), "00:00 CDT"))
return(DF)
}
#
# encode companies - each company gets a code
#
encodeCompanies <- function(DF) {
DF$Company <- apply(FUN = function(row) compHash[[row["Company"]]], X = DF, MARGIN = 1)
return(DF)
}
#
# perform all data parsing in one function
#
parseTripData <- function(DF) {
DF <- DF[colsToKeep]
DF <- renameCols(DF)
DF <- filterTrips(DF)
DF <- encodeCompanies(DF)
DF <- formatDate(DF)
DF <- fixGranularity(DF)
return(DF)
}
######################################################################
#                       "MAIN" METHOD                                #
######################################################################
getData <- function() {
# assign file names for 165 files
filenames <- list()
for (i in 1:165) {
filenames[i] <- paste("./datasets/Taxi_Trips_-_2019_", i, ".tsv", sep='')
}
# get first dataframe with header
DF <- read.table(filenames[[1]], sep = ",", header = TRUE)
DF <- parseTripData(DF)
# append dataframes without header to original dataframe
# ---- MUST START WITH 2 ----
for (i in 2:50) {
# read next csv
temp <- read.table(file = paste0(filenames[[i]]), sep = ",", header = FALSE)
temp <- parseTripData(temp)
print(paste("parsing file: ", i))
# append to main dataframe
DF <- rbind(DF, temp)
}
return(DF)
}
DF <- getData()
#
# SET UP DATA FILE AND EXCLUDE UNWANTED DATA
#
library(dplyr)
library(hash)
library(lubridate)
library(stringr)
source("./companies.r", local = FALSE) # for hashing company names into abbreviations
colsToKeep <- c(3, 5, 6, 9, 10, 17)
easyColNames <- c("start", "duration", "miles", "pickupArea", "dropOffArea", "Company")
######################################################################
#                       HELPER METHODS                               #
######################################################################
#
# remove all trips that do not fall under constraints
#
filterTrips <- function(DF) {
# remove less than 0.5 miles & more than 100 miles
DF <- subset(DF, miles >= .5 & miles <= 100.00)
# remove less than 60 seconds & greater than 5 hours
DF <- subset(DF, duration >= 60 & duration <= 60 * 60 * 5)  # 1-4 312969 rows
# all trips that either start or end outside of a Chicago community area
DF <- subset(DF, !is.na(pickupArea) & !is.na(dropOffArea))  # 1-4 28676 rows
}
#
# give easy names to columns
#
renameCols <- function(DF) {
names(DF) <- easyColNames
return(DF)
}
#
# format to nice date format
#
formatDate <- function(DF) {
DF$start <- strptime(DF$start, format='%m/%d/%Y %I:%M:%S %p')
return(DF)
}
#
# fix trips granularity - 12:15, 12:30, 12:45 become 12:00
#
fixGranularity <- function (DF) {
DF$start <- as.POSIXct(paste0(format(DF$start,"%Y-%m-%d %H:"), "00:00 CDT"))
return(DF)
}
#
# encode companies - each company gets a code
#
encodeCompanies <- function(DF) {
DF$Company <- apply(FUN = function(row) compHash[[row["Company"]]], X = DF, MARGIN = 1)
return(DF)
}
#
# perform all data parsing in one function
#
parseTripData <- function(DF) {
DF <- DF[colsToKeep]
DF <- renameCols(DF)
DF <- filterTrips(DF)
DF <- encodeCompanies(DF)
DF <- formatDate(DF)
DF <- fixGranularity(DF)
return(DF)
}
######################################################################
#                       "MAIN" METHOD                                #
######################################################################
getData <- function() {
# assign file names for 165 files
filenames <- list()
for (i in 1:165) {
filenames[i] <- paste("./datasets/Taxi_Trips_-_2019_", i, ".tsv", sep='')
}
# get first dataframe with header
DF <- read.table(filenames[[1]], sep = ",", header = TRUE)
DF <- parseTripData(DF)
# append dataframes without header to original dataframe
# ---- MUST START WITH 2 ----
for (i in 2:5) {
# read next csv
temp <- read.table(file = paste0(filenames[[i]]), sep = ",", header = FALSE)
temp <- parseTripData(temp)
print(paste("parsing file: ", i))
# append to main dataframe
DF <- rbind(DF, temp)
}
return(DF)
}
DF <- getData()
DF_samp <- sample(DF, 500)
DF_samp <- sample_n(DF, 500)
getBasicBarPlot(parseByHour(DF_samp))
parseByHour(DF_samp)
temp <- aggregate(x = DF_samp, by = list(format(DF_samp$start, "%H")), FUN = sum)
temp <- aggregate(x = list(format(DF_samp$start, "%H")), by = DF_samp, FUN = sum)
temp <- aggregate(x = list(format(DF_samp$start, "%H")), by = DF_samp, FUN = sum)
temp <- aggregate(x = DF_samp$miles, by = list(format(DF_samp$start, "%H")), FUN = sum)
temp <- aggregate(x = DF_samp$miles, by = list(format(DF_samp$start, "%H")), FUN = sum)
temp <- aggregate(x = format(DF_samp$start, "%H"), by = DF_samp, FUN = sum)
#
# SET UP DATA FILE AND EXCLUDE UNWANTED DATA
#
library(dplyr)
library(hash)
library(lubridate)
library(stringr)
source("./companies.r", local = FALSE) # for hashing company names into abbreviations
colsToKeep <- c(3, 5, 6, 9, 10, 17)
easyColNames <- c("start", "duration", "miles", "pickupArea", "dropOffArea", "Company")
######################################################################
#                       HELPER METHODS                               #
######################################################################
#
# remove all trips that do not fall under constraints
#
filterTrips <- function(DF) {
# remove less than 0.5 miles & more than 100 miles
DF <- subset(DF, miles >= .5 & miles <= 100.00)
# remove less than 60 seconds & greater than 5 hours
DF <- subset(DF, duration >= 60 & duration <= 60 * 60 * 5)  # 1-4 312969 rows
# all trips that either start or end outside of a Chicago community area
DF <- subset(DF, !is.na(pickupArea) & !is.na(dropOffArea))  # 1-4 28676 rows
}
#
# give easy names to columns
#
renameCols <- function(DF) {
names(DF) <- easyColNames
return(DF)
}
#
# format to nice date format
#
formatDate <- function(DF) {
DF$start <- strptime(DF$start, format='%m/%d/%Y %I:%M:%S %p')
return(DF)
}
#
# fix trips granularity - 12:15, 12:30, 12:45 become 12:00
#
fixGranularity <- function (DF) {
DF$start <- as.POSIXct(paste0(format(DF$start,"%Y-%m-%d %H:"), "00:00 CDT"))
return(DF)
}
#
# encode companies - each company gets a code
#
encodeCompanies <- function(DF) {
DF$Company <- apply(FUN = function(row) compHash[[row["Company"]]], X = DF, MARGIN = 1)
return(DF)
}
#
# perform all data parsing in one function
#
parseTripData <- function(DF) {
DF <- DF[colsToKeep]
DF <- renameCols(DF)
DF <- sample_n(DF, 100)                           #######################################################################
DF <- filterTrips(DF)
DF <- encodeCompanies(DF)
DF <- formatDate(DF)
DF <- fixGranularity(DF)
return(DF)
}
######################################################################
#                       "MAIN" METHOD                                #
######################################################################
getData <- function() {
# assign file names for 165 files
filenames <- list()
for (i in 1:165) {
filenames[i] <- paste("./datasets/Taxi_Trips_-_2019_", i, ".tsv", sep='')
}
# get first dataframe with header
DF <- read.table(filenames[[1]], sep = ",", header = TRUE)
DF <- parseTripData(DF)
# append dataframes without header to original dataframe
# ---- MUST START WITH 2 ----
for (i in 2:165) {
# read next csv
temp <- read.table(file = paste0(filenames[[i]]), sep = ",", header = FALSE)
temp <- parseTripData(temp)
print(paste("parsing file: ", i))
# append to main dataframe
DF <- rbind(DF, temp)
}
return(DF)
}
getCommunityAreas <- function() {
D <- data.frame(chicago$area_num_1, chicago$community)
names(D) <- c("id", "name")
return(D)
}
DF <- getData()
DF_samp <- DF
shiny::runApp()
chicagoMapData
library(rgdal)
library(leaflet)
library(geojsonR)
library(geojsonio)
chicago <- readOGR("./chicagoMapData/geo_export_bd87ad81-9ee0-4a43-a802-c68e88a02bba.shp",
GDAL1_integer64_policy = TRUE)
runApp()
temp <- subset(DF, DF$pickupArea == "1")
tempagg <- aggregate(x = rep(1, nrow(DF)), by = list(DF$dropoffArea), sum)
tempagg <- aggregate(x = rep(1, nrow(temp)), by = list(temp$dropoffArea), sum)
list(temp$dropoffArea)
temp$dropoffArea
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
View(temp)
temp <- subset(DF, DF$pickupArea == "2")
View(temp)
tempagg <- aggregate(x = rep(1, nrow(temp)), by = list(temp$dropOffArea), sum)
View(tempagg)
runApp()
getCommunityAreas <- function() {
D <- data.frame(chicago$area_num_1, chicago$community)
names(D) <- c("id", "name")
D <- D[order(D$id),]
return(D)
}
communityAreas <- getCommunityAreas()
View(communityAreas)
rm(communityAreas)
runApp()
getCommunityAreas <- function() {
D <- data.frame(strtoi(chicago$area_num_1), chicago$community)
names(D) <- c("id", "name")
D <- D[order(D$id),]
return(D)
}
runApp()
chicago <- readOGR("./chicagoMapData/geo_export_bd87ad81-9ee0-4a43-a802-c68e88a02bba.shp",
GDAL1_integer64_policy = TRUE)
commHash <- hash(chicago$area_num_1, chicago$community)
commUnhash <- hash(chicago$community, chicago$area_num_1)
commHash[["1"]]
commHash[["8"]]
commHash[["24"]]
shiny::runApp()
shiny::runApp()
shiny::runApp()
apply(FUN = function(row) commHash[[row["dropOffArea"]]], X = DF, MARGIN = 1)
temp <- apply(FUN = function(row) commHash[[row["dropOffArea"]]], X = DF, MARGIN = 1)
temp <- apply(FUN = function(row) row["dropOffArea"], X = DF, MARGIN = 1)
View(temp)
temp <- apply(FUN = function(row) typeof(row["dropOffArea"]), X = DF, MARGIN = 1)
runApp()
temp <- aggregate(x = rep(1, nrow(D)), by = commHash[[list(D$dropOffArea)]], sum)
temp <- aggregate(x = rep(1, nrow(DF)), by = commHash[[list(DF$dropOffArea)]], sum)
temp <- aggregate(x = rep(1, nrow(DF)), by = list(DF$dropOffArea), sum)
View(temp)
by__ <- list(DF$dropOffArea)
View(by__)
by__ <- apply(FUN = function(row) commHash[[row["Company"]]], X = DF, MARGIN = 1)
by__ <- apply(FUN = function(row) commHash[[row["dropOffArea"]]], X = DF, MARGIN = 1)
by__ <- list(DF$dropOffArea)
temp <- aggregate(x = rep(1, nrow(DF)), by = list(DF$dropOffArea), sum)
typeof(list(DF$dropOffArea))
commHash[[DF$dropOffArea]]
no <- list(DF$dropOffArea)
View(no)
yes <- apply(FUN = function(row) commHash[[row["Company"]]], X = D, MARGIN = 1)
yes <- apply(FUN = function(row) commHash[[row["Company"]]], X = DF, MARGIN = 1)
yes <- apply(FUN = function(row) print(row["Company"]), X = DF, MARGIN = 1)
yes <- apply(FUN = function(row) print(row["Company"]) == "18", X = DF, MARGIN = 1)
yes <- apply(FUN = function(row) print(row["Company"] == "18"), X = DF, MARGIN = 1)
commHash[["5"]]
yes <- apply(FUN = function(row) commHas[[row["Company"]]], X = DF, MARGIN = 1)
yes <- apply(FUN = function(row) commHash[[row["Company"]]], X = DF, MARGIN = 1)
yes <- apply(FUN = function(row) commHash[[row["Company"]]], X = DF, MARGIN = 2)
yes <- apply(FUN = function(row) unlist(commHash[[row["Company"]]]), X = DF, MARGIN = 1)
yes$f <- apply(FUN = function(row) commHash[[row["Company"]]], X = DF, MARGIN = 1)
yes <- apply(FUN = function(row) commHash[[row["dropOffArea"]]], X = DF, MARGIN = 1)
yes <- apply(FUN = function(row) print(commHash[[row["dropOffArea"]]]), X = DF, MARGIN = 1)
yes <- apply(FUN = function(row) print(commHash[["5"]]), X = DF, MARGIN = 1)
yes <- apply(FUN = function(row) print(commHash[[unlist(row["dropOffArea"])]]), X = DF, MARGIN = 1)
yes <- apply(FUN = function(row) print(typeof(row["dropOffArea"])), X = DF, MARGIN = 1)
yes <- apply(FUN = function(row) print(typeof(unlist(row["dropOffArea"]))), X = DF, MARGIN = 1)
yes <- apply(FUN = function(row) print(typeof(unlist(2))), X = DF, MARGIN = 1)
yes <- apply(FUN = function(row) print(typeof(unlist("2"))), X = DF, MARGIN = 1)
yes <- apply(FUN = function(row) commHash[[ unlist(row["dropOffArea"]) ]], X = DF, MARGIN = 1)
